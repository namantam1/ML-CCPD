{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN:\n",
    "    def __init__(self, neurons=5, epochs=2000, lr=0.5):\n",
    "        self.hidden_layer_neurons = neurons\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(k):\n",
    "        return 1/(1+np.exp(-k))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_derivative(k):\n",
    "        return k * (1 - k)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        inp_layer, out_layer = X.shape[1], Y.shape[1]\n",
    "        hid_layer = self.hidden_layer_neurons\n",
    "        epochs, lr = self.epochs, self.lr\n",
    "\n",
    "        self.hid_weights = np.random.uniform(size=(inp_layer, hid_layer))\n",
    "        self.hid_bias = np.random.uniform(size=(1, hid_layer))\n",
    "\n",
    "        self.out_weights = np.random.uniform(size=(hid_layer, out_layer))\n",
    "        self.out_bias = np.random.uniform(size=(1, out_layer))\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # Forward propogation\n",
    "            hid_layer_out, predicted_out = self._predict(X)\n",
    "\n",
    "            # Backward propogation\n",
    "            error = Y - predicted_out\n",
    "            d_predicted_output = error * self._sigmoid_derivative(predicted_out)\n",
    "\n",
    "            error_hid_layer = d_predicted_output.dot(self.out_weights.T)\n",
    "            d_hid_layer = error_hid_layer * self._sigmoid_derivative(hid_layer_out)\n",
    "\n",
    "            # updating the weights and bias\n",
    "            self.out_weights += (hid_layer_out.T @ d_predicted_output) * lr\n",
    "            self.out_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
    "            self.hid_weights += (X.T @ d_hid_layer) * lr\n",
    "            self.hid_bias += np.sum(d_hid_layer, axis=0, keepdims=True) * lr\n",
    "        \n",
    "        return predicted_out\n",
    "\n",
    "\n",
    "    def _predict(self, X):\n",
    "        hid_layer_activation = (X @ self.hid_weights) + self.hid_bias\n",
    "        hid_layer_out = self._sigmoid(hid_layer_activation)\n",
    "\n",
    "        out_layer_activation = (hid_layer_out @ self.out_weights) + self.out_bias\n",
    "        predicted_out = self._sigmoid(out_layer_activation)\n",
    "        return hid_layer_out, predicted_out\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._predict(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.994626  ],\n",
       "       [0.99461434],\n",
       "       [0.00677232],\n",
       "       [0.00571687]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 0], [0, 1], [0, 0], [1, 1]])\n",
    "Y = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "ann = ANN(epochs=50000, lr=0.5, neurons=4)\n",
    "\n",
    "ann.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "004e3296fb33a75f912db67dfbc804ddcf50611225758f817e2dd7ebe1314606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
