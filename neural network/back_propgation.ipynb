{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "def _sigmoid(X):\n",
    "    return X * (1 - X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[.05, .1]])\n",
    "expected_output = np.array([[.01, .99]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "learning_rate= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurons in layer\n",
    "inp_layer, hid_layer, out_layer = 2, 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weight [0.82869452 0.8957403 ] [0.83687858 0.99951242]\n",
      "Initial hidden bias [0.12896522 0.89887748]\n"
     ]
    }
   ],
   "source": [
    "# Random weight and bias\n",
    "hid_weights = np.random.uniform(size=(inp_layer, hid_layer))\n",
    "hid_bias = np.random.uniform(size=(1, hid_layer))\n",
    "\n",
    "print(\"Initial hidden weight\", *hid_weights)\n",
    "print(\"Initial hidden bias\", *hid_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output weight [0.42183947 0.47128296] [0.28180122 0.18316238]\n",
      "Initial output bias [0.48370691 0.80028129]\n"
     ]
    }
   ],
   "source": [
    "out_weights = np.random.uniform(size=(hid_layer, out_layer))\n",
    "out_bias = np.random.uniform(size=(1, out_layer))\n",
    "\n",
    "print(\"Initial output weight\", *out_weights)\n",
    "print(\"Initial output bias\", *out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weight:  [[0.85674304 0.92243072]\n",
      " [0.89297562 1.05289326]]\n",
      "Final hidden bias:  [[0.6899356  1.43268588]]\n",
      "Final output weights:  [[-1.19133627  1.43983827]\n",
      " [-1.79926643  1.4154113 ]]\n",
      "Final output bias:  [[-2.23582584  2.38211873]]\n",
      "Output from neural network:  [[0.01039053 0.98961363]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    # Forward propogation\n",
    "    hid_layer_activation = (inputs @ hid_weights) + hid_bias\n",
    "    hid_layer_out = sigmoid(hid_layer_activation)\n",
    "\n",
    "    out_layer_activation = (hid_layer_out @ out_weights) + out_bias\n",
    "    predicted_out = sigmoid(out_layer_activation)\n",
    "\n",
    "    # Backward propogation\n",
    "    error = expected_output - predicted_out\n",
    "    d_predicted_output = error * _sigmoid(predicted_out)\n",
    "\n",
    "    error_hid_layer = d_predicted_output.dot(out_weights.T)\n",
    "    d_hid_layer = error_hid_layer * _sigmoid(hid_layer_out)\n",
    "\n",
    "    # updating the weights and bias\n",
    "    out_weights += (hid_layer_out.T @ d_predicted_output) * learning_rate\n",
    "    out_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    hid_weights += (inputs.T @ d_hid_layer)  * learning_rate\n",
    "    hid_bias += np.sum(d_hid_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "print(\"Final hidden weight: \", hid_weights)\n",
    "print(\"Final hidden bias: \", hid_bias)\n",
    "\n",
    "print(\"Final output weights: \", out_weights)\n",
    "print(\"Final output bias: \", out_bias)\n",
    "\n",
    "print(\"Output from neural network: \", predicted_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01594208, 0.9842288 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ANN:\n",
    "    def __init__(self, neurons=5, epochs=2000, lr=0.5):\n",
    "        self.hidden_layer_neurons = neurons\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(k):\n",
    "        return 1/(1+np.exp(-k))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_derivative(k):\n",
    "        return k * (1 - k)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        inp_layer, out_layer = X.shape[1], Y.shape[1]\n",
    "        hid_layer = self.hidden_layer_neurons\n",
    "        epochs, lr = self.epochs, self.lr\n",
    "\n",
    "        self.hid_weights = np.random.uniform(size=(inp_layer, hid_layer))\n",
    "        self.hid_bias = np.random.uniform(size=(1, hid_layer))\n",
    "\n",
    "        self.out_weights = np.random.uniform(size=(hid_layer, out_layer))\n",
    "        self.out_bias = np.random.uniform(size=(1, out_layer))\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # Forward propogation\n",
    "            hid_layer_out, predicted_out = self._predict(X)\n",
    "\n",
    "            # Backward propogation\n",
    "            error = Y - predicted_out\n",
    "            d_predicted_output = error * self._sigmoid_derivative(predicted_out)\n",
    "\n",
    "            error_hid_layer = d_predicted_output.dot(self.out_weights.T)\n",
    "            d_hid_layer = error_hid_layer * self._sigmoid_derivative(hid_layer_out)\n",
    "\n",
    "            # updating the weights and bias\n",
    "            self.out_weights += (hid_layer_out.T @ d_predicted_output) * lr\n",
    "            self.out_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n",
    "            self.hid_weights += (X.T @ d_hid_layer)  * lr\n",
    "            self.hid_bias += np.sum(d_hid_layer, axis=0, keepdims=True) * lr\n",
    "        \n",
    "        return predicted_out\n",
    "\n",
    "\n",
    "    def _predict(self, X):\n",
    "        hid_layer_activation = (X @ self.hid_weights) + self.hid_bias\n",
    "        hid_layer_out = self._sigmoid(hid_layer_activation)\n",
    "\n",
    "        out_layer_activation = (hid_layer_out @ self.out_weights) + self.out_bias\n",
    "        predicted_out = self._sigmoid(out_layer_activation)\n",
    "        return hid_layer_out, predicted_out\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._predict(X)[1]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "\n",
    "\n",
    "ann = ANN()\n",
    "\n",
    "ann.fit(inputs, expected_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "004e3296fb33a75f912db67dfbc804ddcf50611225758f817e2dd7ebe1314606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
